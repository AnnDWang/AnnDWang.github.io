---
layout: mypost
title: 神经网络
categories: [Machine Learning]
---

> 内容来自于[《机器学习》](https://book.douban.com/subject/26708119/)

# 神经元模型

神经网络是由具有适应性的简单单元组成的广泛并行互联的网络，它的组织能够模拟生物神经系统对真实世界物体所作出的交互反应。

神经网络中最基本的成分是神经元模型，即上述定义中的“简单单元”。

M-P神经元模型：在这个模型中，神经元接收到来自n个其他神经元传递过来的输入信号，这些输入信号通过带权重的连接进行传递，神经元接收到的总输入值将与神经元的阈值进行比较，然后通过激活函数处理以产生神经元的输出。

![](1.png)

理想的激活函数是阶跃函数，它将输入值映射为输出值“0”或“1”，显然1对应于神经元兴奋，“0”对应于神经元抑制。然而，阶跃函数具有不连续、不光滑等不太好的性质，因此实际常用Sifmoid函数作为激活函数。把可能在较大范围内变化的输入值挤压到（0,1）输出值范围内，因此有时也称为“挤压函数”。

把许多个这样的神经元按一定的层次结构连接起来，就得到了神经网络。

# 感知机与多层网络

感知机由两层神经元组成，输入层接收外界输入信号后传递给输出层，输出层是M-P神经元，亦称为“阈值逻辑单元”.

感知机能够很容易地实现逻辑与、或、非运算。

![](2.png)

一般来说，给定训练数据集，权重w和阈值theta可通过学习得到

阈值theta可以看做一个固定输入为-1.0的哑结点所对应的连接权重wn+1，这样，权重和阈值的学习就可以统一为权重的学习。

![](3.png)

感知机的预测如果是正确的，感知机不发生变化，否则根据错误的程度进行权重调整。

需要注意的是，感知机只有输出神经元进行激活函数处理，即只有一层功能神经元，其学习能力非常有限。事实上，上述与、或、非问题都是线性可分的问题。可以证明，若两类模式是线性可分的，即存在一个线性超平面能将它们分开。则感知机的学习过程一定会收敛而求得适当的权重，否则感知机学习过程将会发生振荡，w难以稳定下来，不能求得合适的解。

要解决非线性可分问题，需要考虑多层功能神经元。输入层与输出层之间的一层神经元，被称为隐层或者隐含层。隐含层和输出层神经元都是拥有激活函数的功能神经元。

一般的，常见的神经网络是层级结构，每层神经元与下层神经元全互连，神经元之间不存在同层连接，也不存在跨层连接，这样的神经网络通常称为多层前馈神经网络。输入层神经元接收外界输入，隐层与输出层神经元对信号进行加工，最终结果由输出层神经元输出；换言之，输入层神经元仅仅是接受输入，不进行函数处理，隐层与输出层包含功能神经元。

![](4.png)

只需包含隐层，即可成为多层网络。神经网络的学习过程，就是根据训练数据来调整神经元之间的连接全，以及每个功能神经元的阈值。即神经网络学到的东西，蕴含在连接权与阈值中。

# 误差逆传播算法

多层网络的学习能力比单层感知机强的多欲训练多层网络，简单感知机的学习规则显然不够，需要更强大的算法，误差逆传播（BackPropagation，简称BP）算法就是其中最杰出的代表，是迄今为止最成功的神经网络学习算法。

现实任务重使用神经网络时，大多是在使用BP算法进行训练。值得指出的是，BP算法不仅可用于多层前馈神经网络，还可用于其他类型的神经网络，例如训练递归神经网络。但是通常说BP网络时，一般是指用BP算法训练的多层前馈神经网络。

对每个训练样例，BP算法执行以下操作：先将输入实例提供给输入层神经元，然后逐层将信号前传，知道产生输入层的结果，然后计算输出层的误差，再将误差逆向传播至隐层神经元，最后根据隐层神经元的误差来对连接权和阈值进行调整，该迭代过程循环进行，直到达到某些条件为止。

BP算法的目标是要最小化训练集D上的累积误差

但是上述的 标准BP算法 每次仅仅针对一个训练样例更新连接权和阈值，也就是说更新规则是基于单个的Ek推导得到的。
如果类似地推导出基于累积误差最小化的更新规则，就得到了累积误差逆传播(accumulated error backpropagation)算法。累积BP算法与标准BP算法都很常用。
一般来说，标准BP算法每次更新只针对单个样例，参数更新得非常频繁，而且对不同样例进行更新的效果可能出现抵消现象，因此为了达到同样的累积误差极小点，标准BP算法往往需要进行更多次迭代，累积BP算法直接针对累积误差最小化，它在读取整个训练集D一遍后才对参数进行更新，其参数更新的频率低得多。但在很多任务中，累积误差下降到一定程度之后，进一步下降会非常缓慢，这是标准BP往往会更快获得较好的解，尤其是在训练集D非常大时更明显。

只需一个包含足够多神经元的隐层，多层前馈网络就能以任意精度逼近任意复杂度的连续函数。然而，如何设置隐层神经元的个数仍是个未决问题，实际应用中，通常靠“试错法”调整。

BP神经网络经常遭遇过拟合，训练误差持续降低，但是测试误差却可能上升。两种策略缓解BP网络的过拟合。

一是早停，将数据分成训练集合验证集，训练集用来计算梯度、更新连接权和阈值，验证集用来估计误差，若训练集误差降低但验证集误差升高，则停止训练，同时返回具有最小验证集误差的连接权和阈值。

第二种策略是正则化，基本思想是在误差目标函数中增加一个用于描述网络复杂度的部分，例如连接权与阈值的平方和。

# 全局最小与局部极小

误差函数的局部极小值和全局最小值

如果误差函数有多个局部极小，则不能保证找到的解是全局最小，对后一种情形，我们称参数寻优陷入了局部极小，这显然不是我们所希望的。

在现实任务中，人们常采用以下策略来试图跳出局部极小，从而进一步接近全局：

+ 以多组不同参数值初始化多个神经网络，按标准方法训练后，取其中误差最小的解作为最终参数。这相当于从多个不同的初始点开始搜索，这样就可能陷入不同的局部极小，从中进行选择有可能获得更接近全局最小的结果。
+ 使用模拟退火技术，模拟退火在每一步都以一定的概率接受比当前解更差的结果，从而有助于跳出局部最小。在每步迭代过程中，接受次优解的概率随着时间的推移而逐渐降低，从而保证算法稳定。
+ 使用随机梯度下降，与标准梯度下降法精确计算梯度不同，随机梯度下降法在计算梯度时加入了随机因素。即便陷入局部极小点，他计算出的梯度仍可能不为0，这样就有机会跳出局部极小继续搜索。

遗传算法也通常用来训练神经网络以更好地逼近全局最小。需要注意的是，上述用于跳出局部极小的技术大多是启发式，理论上缺乏保障。

# 其他常见神经网络

RBF（Radial Basis Function）网络

竞争型学习（competitive learning）是神经网络中一种常用的无监督学习策略，在使用该策略时，网络的输出神经元相互竞争，每一时刻仅有一个竞争获胜的神经元被集火，其他神经元的状态被移植，这种机制亦称为胜者通吃原则。

ART（adaptive Resonance Theory,自适应谐振理论）网络是竞争型学习的重要代表.

SOM（Self-Organizing Map，自组织映射）网络是一种竞争学习型的无监督神经网络，能将高维输入数据映射到低维控件，同时保持输入数据在高维空间的拓扑结构，即将高维空间中相似的样本点映射到网络输出层中的邻近神经元。

递归神经网络允许网络中出现环形结构，从而可让一些神经元的输出反馈回来作为输入信号。这样的结构与信息反馈过程，使得网络在t时刻的输出状态不仅与t时刻的输入有关，还与t-1时刻的网络状态有关，从而能处理与时间有关的动态变化。

Elman网络是最常用的递归神经网络之一。

Boltzmann机

神经网络中有一类模型是我网络状态定义一个能量，能量最小化时网络达到理想状态，而网络的训练就是在最小化这个能量函数。

Boltzmann机就是一种基于能量的模型，其神经元分为两层：显层与隐层，显层用于表示数据的输入与输出，隐层则被理解为数据的内在表达。

受限Boltzmann机

# 深度学习










