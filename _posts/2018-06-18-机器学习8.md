---
layout: mypost
title: 集成学习
categories: [Machine Learning]
---

> 内容来自于[《机器学习》](https://book.douban.com/subject/26708119/)

# 个体与集成

集成学习通过构建并结合多个学习器来完成学习任务，有时也被成为多分类器系统（multi-classifier system）、基于委员会的学习(committee-based learning)等。

集成学习的一般结构为，先产生一组个体学习器，再用某种策略将它们结合起来。个体学习器通常由一个现有的学习算法从训练数据产生，此时集成中只包含同种类型的个体学习器，例如决策树集成中全是决策树，这样的集成是同质的。同质集成中的个人学习器亦称为基学习器，相应的学习算法称为基学习算法。集成也可以包含不同类型的个体学习器，这样的集成是异质的。异质集成中的个体学习器由不同的学习算法生成，这时就不再有基学习算法。相应的，个体学习器不称为基学习器，常称为组件学习器或者直接称为个体学习器。

![](1.png)

集成学习通过将多个学习器进行结合，常课获得比单一学习器显著优越的泛化性能。这对弱学习器尤为明显，因此集成学习的很多理论都是针对弱学习器进行的。而基学习器有时候也被直接称为弱学习器，但需要注意的是，虽然从理论上来说，使用弱学习器集成足以获得好的性能，但在实践中出于种种考虑，例如希望使用较少的个体学习器，或是重用关于常见学习器的一些经验等，人们往往会使用比较强的学习器。

要获得好的集成，个体学习器应该“好而不同”，即个体学习器要有一定的准确性，即学习器不能太坏，并且要有多样性，即学习器间具有差异。

个体学习器的准确性和多样性本身存在冲突，一般的，准确性很高之后，要增加多样性就需要牺牲准确性，事实上，如何产生并结合好而不同的个体学习器，恰是集成学习研究的核心。

根据个体学习器的生成方式，目前的集成学习方法大致可分为两类，即个体学习器间存在强依赖关系，必须串行生成的序列化方法，以及个体学习器间不存在强依赖关系，可同时生成的并行化方法；前者的代表是Boosting，后者的代表是Bagging和随机森林（Random Forest）。

# Boosting

Boosting 是一族可将弱学习器提升为强学习器的算法，这族算法的工作机制类似：先从初始训练集训练出一个基学习器，在根据基学习器的表现对训练样本分布进行调整，使得闲钱学习器做错的训练样本在后续收到更多关注，然后基于调整后的样本分布来训练下一个基学习器；如此重复进行，直至基学习器数目达到事先指定的值T，最终将这T个基学习器进行加权结合。

Boosting族算法最著名的代表是AdaBoost,算法描述如下：

![](2.png)

其中，yi属于{-1，+1},f是真实函数

AdaBoost有多种推导方式，比较容易理解的是基于加性模型（additive model）即基于学习器的线性组合：

![](3.png)

来最小化指数损失函数：

![](4.png)

若指数损失函数最小化，则分类错误率也将最小化，这说明指数损失函数是分类任务原本0/1损失函数的一致的替代损失函数。由于这个替代函数有更好的数学性质，因此用它替代0/1损失函数作为优化目标、

Boosting 算法要求基学习器能对特定的数据分布进行学习，这可通过重赋权法 实施，即在训练过程的每一轮中，根据样本分布为每个训练样本重新赋予一个权重。对无法接受带权样本的基学习算法，则可通过重采样法来处理，即在每一轮学习中，根据样本分布对训练集重新进行采样，在用重采样而得的样本集对基学习器进行训练。一般而言，这两种做法没有显著的优劣差别。需要注意的是，Boosting算法在训练的每一轮都要检查当前生成的基学习器是否满足基本条件，一旦条件不满足，则当前基学习器即被抛弃，且学习过程停止，在此种情形下，初始设置的学习轮数也许还远未达到，可能导致最终集成中只包含很少的基学习器而性能不佳，若采用重采样法，则可获得重启动的机会，避免训练过程过早停止，即在抛弃不满足条件的当前学习器之后，可根据当前分布重新对训练样本进行采样，再基于新的采样结果重新训练处基学习器，从而使得学习过程可以持续预设的T论完成。

从偏差--方差分解的角度看，Boosting主要关注降低偏差，因此Boosting能基于泛化性能相当若的学习器构建出很强的集成。

# Bagging与随机森林

遇得到泛化性能强的集成，集成中的个体学习器应尽可能相互独立，虽然独立在现实任务中无法做到，但可以设法使几学期尽可能具有较大的差异。给定一个训练数据集，一种可能的做法是对训练样本进行采样，产生若干不同的自己，再从每个数据自己种训练处一个基学习器，这样，由于训练数据不同，我们获得的基学习器可能具有比较大的差异，然而，为获得好的集成，我们同时还希望个体学习器不能太差。如果采样出的每个子集都完全不同，则每个基学习器都只用到了一小部分训练数据，甚至不足以进行有效学习，这显然无法确保产生出比较好的基学习器。为了解决这个问题，我们可考虑使用相互有交叠的采样自子集。


















