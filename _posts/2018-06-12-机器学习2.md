---
layout: mypost
title: 模型评估与选择
categories: [Machine Learning]
---

> 内容来自于[《机器学习》](https://book.douban.com/subject/26708119/)

# 经验误差与过拟合

学习器在训练集上的误差称为“训练误差”或者“经验误差”，在新样本上的误差称为“泛化误差”。

过拟合 欠拟合

过拟合是机器学习面临的关键障碍

# 评估方法

### 留出法

留出法直接将数据集D划分为两个互斥的集合，其中一个集合作为训练集S，另一个作为测试集T。在S上训练处模型之后，用T来评估其测试误差，作为对泛化误差的估计。

### 交叉验证法

交叉验证法将数据集D划分为k个大小相似的互斥子集，每个子集都尽可能保持数据分布的一致性，每次只用k-1个子集的并集作为训练集，余下的那个子集作为测试集。这样就可以获得k组训练集测试集。从而可以进行k次训练和测试，最终返回的是k个测试结果的均值。

交叉验证法评估结果的稳定性和保真性很大程度上取决于k的取值。通常把交叉验证法成为k折交叉验证或者k倍交叉验证。

家丁数据集D中包含m个样本，若令k=m，则得到了交叉验证法的一个特例，留一法。

### 自助法

留出法和交叉验证法中，由于保留了一部分样本用于测试，因此实际评估的模型所使用的训练集比D小，这必然会引入一些因训练样本规模不同而导致的估计偏差。

自助法直接以自主采样法为基础。给定包含m个样本的数据集D，我们对其进行采样产生数据集D'，每次随机从D中挑选一个样本，将其拷贝放入D'，然后将该样本放回初始数据集D中，使得该样本在下次采样时仍有可能被采到，这个过程重复执行m次后，我们就得到了包含m个样本的数据集D'。这就是自助采样的结果。

显然，D中有一部分样本会在D'中多次出现，而另一部分样本不出现，可以做一个简单估计，样本在m次采样中始终不被采样到的概率是（1-1/m）^m，取极限得到约为0.368

即通过自助采样，初始数据集中约有36.8%的样本未出现在采样数据及D'中，于是我们可以将D’用作训练集，D\D'用作测试集，这样，实际评估的模型与期望评估的模型都使用m个训练样本，而我们让人有数据总量的1/3的、没在训练集中出现的样本用作调试，这样的测试结果，也称为“包外估计”。

自助法在数据集较小，难以有效划分训练/测试集时很有用，此外，自助法能从初始数据集中产生多个不同的训练集，这对集成学习等方法有很大好处。然而，自助法产生的数据集改变了初始数据集的分布，这回引入估计偏差。

### 调参与最终模型。

通常把学得模型在实际使用中遇到的数据称为测试数据，为了加以区分，模型评估与选择中用于评估测试的数据集长称为验证集。在研究对比不同算法的泛化性能时，我们用测试集上的判别效果来估计模型在实际使用时的泛化能力，二八训练数据另外划分为训练集和验证集，并用验证集上的性能来进行模型选择和调参。

# 性能度量

回归任务最常用的性能度量是均方误差 mean squared error

错误率是分类错误的样本数占样本总数的比例，精度则是分类正确的样本数栈样本总数的比例。

查准率（precision）

查全率（recall）

TP(真正例)

FN（假反例）

FP（假正例）

TN（真反例）

查准率=TP/(TP+FP)

查全率=TP/(TP+FN)

以查准率为纵轴，查全率为横轴作图，得到查准率、查全率曲线，简称P-R曲线。

平衡点BEP是在查准率=查全率时的取值。

更加常用的是F1度量

F1=2X查准率X查全率/(查准率+查全率)

### ROC与AUC

ROC全称是受试者工作特征曲线。ROC曲线的纵轴是真正例率TPR，横轴是假正例率 FPR，两者分别定义为

TPR=TP/(TP+FN)

FPR=FP/(TN+FP)

ROC曲线下的面积为AUC

# 比较检验


















